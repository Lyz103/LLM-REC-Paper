# Daily papers' abstract
**2024-04-02**  
**✅IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT**  
Multimodal foundation models are transformative in sequential recommender systems, leveraging powerful representation learning capabilities. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation models for recommendation tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed. Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for Multimodal Representation), a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.
IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art PEFT. More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for multimodal sequential recommendation tasks. Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT. This is also a notable improvement over the Adapter and LoRA, which require 37-39 GB GPU memory and 350-380 seconds per epoch for training.
Furthermore, we propose a new composite efficiency metric, TPME (Training-time, Parameter, and GPU Memory Efficiency) to alleviate the prevalent misconception that "parameter efficiency represents overall efficiency". TPME provides more comprehensive insights into practical efficiency comparisons between different methods. Besides, we give an accessible efficiency analysis of all PEFT and FFT approaches, which demonstrate the superiority of IISAN. We release our codes and other materials at this https URL.  
我们的论文介绍了IISAN（Intra- and Inter-modal Side Adapted Network for Multimodal Representation），这是一种简单的即插即用架构，使用了分离的PEFT结构，并利用了模态内部和模态间的适应性，以解决多模态序列推荐任务中GPU内存效率和训练速度的问题。IISAN能够达到全量微调（FFT）和最先进PEFT的性能水平，并且显著减少了GPU内存使用量，从47GB降低到了仅3GB。此外，它还加快了每个epoch的训练时间，从443秒缩短到了22秒。与需要37-39 GB GPU内存和350-380秒每个epoch训练时间的Adapter和LoRA相比，这也是一个显著的改进。

我们还提出了一个新的复合效率指标TPME（Training-time, Parameter, and GPU Memory Efficiency），以消除“参数效率代表总体效率”的普遍误解。TPME提供了对不同方法的更全面的实际效率比较。此外，我们对所有PEFT和FFT方法进行了可访问的效率分析，展示了IISAN的优越性。我们在此https URL发布了我们的代码和其他材料。

**✅Where to Move Next: Zero-shot Generalization of LLMs for Next POI Recommendation**  
Next Point-of-interest (POI) recommendation provides valuable suggestions for users to explore their surrounding environment. Existing studies rely on building recommendation models from large-scale users' check-in data, which is task-specific and needs extensive computational resources. Recently, the pretrained large language models (LLMs) have achieved significant advancements in various NLP tasks and have also been investigated for recommendation scenarios. However, the generalization abilities of LLMs still are unexplored to address the next POI recommendations, where users' geographical movement patterns should be extracted. Although there are studies that leverage LLMs for next-item recommendations, they fail to consider the geographical influence and sequential transitions. Hence, they cannot effectively solve the next POI recommendation task. To this end, we design novel prompting strategies and conduct empirical studies to assess the capability of LLMs, e.g., ChatGPT, for predicting a user's next check-in. Specifically, we consider several essential factors in human movement behaviors, including user geographical preference, spatial distance, and sequential transitions, and formulate the recommendation task as a ranking problem. Through extensive experiments on two widely used real-world datasets, we derive several key findings. Empirical evaluations demonstrate that LLMs have promising zero-shot recommendation abilities and can provide accurate and reasonable predictions. We also reveal that LLMs cannot accurately comprehend geographical context information and are sensitive to the order of presentation of candidate POIs, which shows the limitations of LLMs and necessitates further research on robust human mobility reasoning mechanisms.  
下一个感兴趣点（POI）推荐为用户提供了有价值的建议，帮助他们探索周围环境。现有研究依赖于从大规模用户签到数据构建推荐模型，这是一项特定任务，并需要大量计算资源。最近，预训练的大型语言模型（LLMs）在各种自然语言处理任务中取得了重大进展，并且也已经在推荐场景中进行了研究。然而，LLMs的泛化能力在解决下一个POI推荐任务时仍未被探索，其中需要提取用户的地理移动模式。虽然有研究利用LLMs进行下一个项目的推荐，但它们未能考虑地理影响和序列转换。因此，它们无法有效解决下一个POI推荐任务。为此，我们设计了新颖的提示策略，并进行了实证研究，以评估LLMs（如ChatGPT）预测用户下一个签到的能力。具体而言，我们考虑了人类移动行为中的几个重要因素，包括用户的地理偏好、空间距离和序列转换，并将推荐任务形式化为排名问题。通过对两个广泛使用的真实世界数据集进行大量实验，我们得出了几个关键发现。实证评估表明，LLMs具有有前途的零-shot推荐能力，并且可以提供准确合理的预测。我们还揭示了LLMs无法准确理解地理背景信息，并且对候选POI的呈现顺序敏感，这显示了LLMs的局限性，需要进一步研究稳健的人类移动推理机制。   
**✅EEG-SVRec: An EEG Dataset with User Multidimensional Affective Engagement Labels in Short Video Recommendation**  
In recent years, short video platforms have gained widespread popularity, making the quality of video recommendations crucial for retaining users. Existing recommendation systems primarily rely on behavioral data, which faces limitations when inferring user preferences due to issues such as data sparsity and noise from accidental interactions or personal habits. To address these challenges and provide a more comprehensive understanding of user affective experience and cognitive activity, we propose EEG-SVRec, the first EEG dataset with User Multidimensional Affective Engagement Labels in Short Video Recommendation. The study involves 30 participants and collects 3,657 interactions, offering a rich dataset that can be used for a deeper exploration of user preference and cognitive activity. By incorporating selfassessment techniques and real-time, low-cost EEG signals, we offer a more detailed understanding user affective experiences (valence, arousal, immersion, interest, visual and auditory) and the cognitive mechanisms behind their behavior. We establish benchmarks for rating prediction by the recommendation algorithm, showing significant improvement with the inclusion of EEG signals. Furthermore, we demonstrate the potential of this dataset in gaining insights into the affective experience and cognitive activity behind user behaviors in recommender systems. This work presents a novel perspective for enhancing short video recommendation by leveraging the rich information contained in EEG signals and multidimensional affective engagement scores, paving the way for future research in short video recommendation systems.  
近年来，短视频平台广受欢迎，因此视频推荐质量对于留住用户至关重要。现有的推荐系统主要依赖于行为数据，但由于数据稀疏性以及意外交互或个人习惯等问题导致的噪音，推断用户偏好时面临一定的限制。为了解决这些挑战并提供对用户情感体验和认知活动更全面的理解，我们提出了EEG-SVRec，这是首个具有短视频推荐中用户多维情感参与标签的EEG数据集。该研究涉及30名参与者，收集了3,657次互动，提供了一个丰富的数据集，可用于更深入地探索用户偏好和认知活动。通过结合自我评估技术和实时、低成本的EEG信号，我们提供了对用户情感体验（情绪、唤醒度、沉浸感、兴趣、视觉和听觉）以及其行为背后的认知机制更详细的理解。我们通过推荐算法的评分预测建立了基准，并展示了加入EEG信号后的显著改进。此外，我们展示了该数据集在洞察推荐系统中用户行为背后的情感体验和认知活动方面的潜力。这项工作通过利用EEG信号和多维情感参与分数中蕴含的丰富信息，提出了一个增强短视频推荐的新视角，为未来短视频推荐系统的研究铺平了道路。




  
