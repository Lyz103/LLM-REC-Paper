# ğŸ”¥ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation



## Motivation
LLMåœ¨æ¨èé¢†åŸŸä¸­æ— æ³•ä»é•¿ç”¨æˆ·è¡Œä¸ºåºåˆ—çš„æ–‡æœ¬ä¸Šæå–æœ‰ç”¨ä¿¡æ¯ï¼Œå³ä½¿æ–‡æœ¬é•¿åº¦è¿œè¿œæ²¡æœ‰è¾¾åˆ°LLMçš„æ–‡æœ¬é•¿åº¦é™åˆ¶ã€‚
<img src="./asset/figure1.png" width="500px">


## Introduction

 å¯¹é›¶æ ·æœ¬ï¼Œæå‡ºäº†SUBRï¼Œç”¨æœ€è¿‘çš„Kä¸ªè¯­ä¹‰æœ€æ¥è¿‘çš„æ¥åšæˆªæ–­ï¼Œè€Œä¸æ˜¯ä»…ä»…æ˜¯æœ€è¿‘çš„Kä¸ªã€‚å¯¹few-shot, é™¤äº†ç”¨SUBRæ¥æå‡æ•°æ®è´¨é‡ï¼Œåˆæå‡ºäº†REtiæ¥ä¿ƒè¿›LLMè§£å†³é•¿åºåˆ—è¡¨ç°å·®ã€‚  
ä¸»è¦è´¡çŒ®ï¼š  
1.ç¡®å®šå¹¶å®Œå–„äº†æ¨èé¢†åŸŸä¸­LLMsï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„ç»ˆèº«åºåˆ—è¡Œä¸ºç†è§£é—®é¢˜  
2.RElla + SUBR + ReiT  
3.few-shot in Rella > full-shot with baseline model  


## Preliminaries

$(x_i, y_i)$ --> ("item", "YES/NO")  
<img src="./asset/figure2.png" width="500px">  
K = 4 in figure 2 (4 items)  


## Method


 ## appendix
 [[CTRä»»åŠ¡]](https://zhuanlan.zhihu.com/p/372110635)

**Q1:** what is zero-shot and few-shot recommendation?  
**A1:** Zero-shot recommendation implies that a model is directly em
ployed for the target recommendation task without any tuning on
 the in-domain training data. Apparently, traditional recommenda
tion models are incapable of accomplishing zero-shot recommen
dation tasks, since they are randomly initialized. However, LLMs
 possess a vast volume of open-world knowledge and logical rea
soning abilities, which enable them to infer the userâ€™s preference
 towards a certain target item based on the profile of user/item.
 Few-shot recommendation refers to low-resource scenarios with
 ğ‘ training data samples. ğ‘ denotes the number of shots, which is
 a relatively small number. This highly requires the data efficiency
 characteristic of an algorithm to fully exploit the limited number of
 training samples to achieve better recommendation performance.
Extending from the definition of few-shot recommendation, we
 can therefore define full-shot recommendation as the setting where
 we train the model based on the entire training set.


